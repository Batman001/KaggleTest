{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunchao/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/sunchao/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import sys, os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/sunchao/Desktop/Guo/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./X_train.csv\", header=0)\n",
    "feature_columns = [\"MYCT\",\"MMIN\",\"MMAX\",\"CACH\",\"CHMIN\",\"CHMAX\"]\n",
    "# remove id columns\n",
    "train_data = train_data.iloc[:,1:]\n",
    "# remove null columns\n",
    "train_data.drop(['Unnamed: 7'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunchao/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/Users/sunchao/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# split the train data of train-data and validation-data\n",
    "X_train,X_val,y_train,y_val = train_test_split(train_data[feature_columns],train_data.PRP,train_size=0.9)\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = scale(X_train)\n",
    "\n",
    "X_val = np.asarray(X_val)\n",
    "X_val = scale(X_val)\n",
    "\n",
    "y_train = np.asarray(y_train).reshape((-1,1))\n",
    "y_train = scale(y_train)\n",
    "\n",
    "y_val = np.asarray(y_val).reshape((-1,1))\n",
    "# y_val = scale(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自变量---源数据: (168, 6) ；  训练集: (151, 6) ；  验证集: (17, 6)\n",
      "因变量---源数据: (168,) ；  训练集: (151, 1) ；  验证集: (17, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"自变量---源数据:\",train_data[feature_columns].shape, \"；  训练集:\",X_train.shape, \"；  验证集:\",X_val.shape)\n",
    "print(\"因变量---源数据:\",train_data.PRP.shape, \"；  训练集:\",y_train.shape, \"；  验证集:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 网络添加函数\n",
    "def add_layer(inputs, input_size, output_size, activation_function=None):\n",
    "    with tf.variable_scope(\"Weights\"):\n",
    "        Weights = tf.Variable(tf.random_normal(shape=[input_size,output_size]), name=\"weights\")\n",
    "    with tf.variable_scope(\"biases\"):\n",
    "        biases = tf.Variable(tf.zeros(shape=[1, output_size]) + 0.1, name=\"biases\")\n",
    "    with tf.name_scope(\"Wx_plus_b\"):\n",
    "        Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob=keep_prob_s)\n",
    "    if activation_function is None:\n",
    "        return Wx_plus_b\n",
    "    else:\n",
    "        return activation_function(Wx_plus_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义占位符和网络层数\n",
    "xs = tf.placeholder(shape = [None, X_train.shape[1]], dtype=tf.float32, name=\"inputs\")\n",
    "ys = tf.placeholder(shape = [None, 1], dtype=tf.float32, name=\"y_true\")\n",
    "keep_prob_s = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"layer_1\"):\n",
    "    l1 = add_layer(xs, 6, 100, activation_function=tf.nn.relu)\n",
    "\n",
    "with tf.name_scope(\"hidder_layer1\"):\n",
    "    h_layer1 = add_layer(l1, 100, 1000, activation_function=tf.nn.relu)\n",
    "        \n",
    "with tf.name_scope(\"hidder_layer2\"):\n",
    "    h_layer2 = add_layer(h_layer1, 1000, 100, activation_function=tf.nn.relu)  \n",
    "\n",
    "with tf.name_scope(\"hidder_layer3\"):\n",
    "    h_layer3 = add_layer(h_layer2, 100, 10, activation_function=tf.nn.relu)  \n",
    "\n",
    "with tf.name_scope(\"y_pred\"):\n",
    "    pred = add_layer(h_layer3, 10, 1)\n",
    "\n",
    "# 保存pred的操作结果，做恢复使用\n",
    "pred = tf.add(pred, 0, name=\"pred\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - pred), reduction_indices=[1]))   # MSE误差\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X,y,n, keep_prob):\n",
    "    init = tf.global_variables_initializer()\n",
    "    feed_dict = {xs:X, ys:y, keep_prob_s:keep_prob}\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=15)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(logdir=\"nn_log\", graph=sess.graph)  #写tensorbord\n",
    "        sess.run(init)\n",
    "        for i in range(n):\n",
    "            _loss, _ = sess.run([loss,train_op], feed_dict=feed_dict)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(\"epoch:%d\\tloss:%.5f\" % (i, _loss))\n",
    "                y_pred = sess.run(pred, feed_dict = feed_dict)\n",
    "                rs = sess.run(merged, feed_dict=feed_dict)\n",
    "                writer.add_summary(summary=rs, global_step=i)  #写tensorbord\n",
    "                saver.save(sess=sess, save_path=\"nn_model/nn.model\", global_step=i) # 保存模型\n",
    "        saver.save(sess=sess, save_path=\"nn_model/nn.model\", global_step=n)  # 保存模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\tloss:3087697.25000\n",
      "epoch:100\tloss:183.64865\n",
      "epoch:200\tloss:41.75545\n",
      "epoch:300\tloss:14.50969\n",
      "epoch:400\tloss:5.87180\n",
      "epoch:500\tloss:2.80568\n",
      "epoch:600\tloss:1.79247\n",
      "epoch:700\tloss:1.35292\n",
      "epoch:800\tloss:1.11992\n",
      "epoch:900\tloss:0.99900\n",
      "epoch:1000\tloss:0.93859\n",
      "epoch:1100\tloss:0.90704\n",
      "epoch:1200\tloss:0.88981\n",
      "epoch:1300\tloss:0.87997\n",
      "epoch:1400\tloss:0.87396\n",
      "epoch:1500\tloss:0.86857\n",
      "epoch:1600\tloss:0.86483\n",
      "epoch:1700\tloss:0.86831\n",
      "epoch:1800\tloss:0.85937\n",
      "epoch:1900\tloss:0.85699\n",
      "epoch:2000\tloss:0.85446\n",
      "epoch:2100\tloss:0.85201\n",
      "epoch:2200\tloss:0.84979\n",
      "epoch:2300\tloss:0.84770\n",
      "epoch:2400\tloss:0.84546\n",
      "epoch:2500\tloss:0.84515\n",
      "epoch:2600\tloss:0.84171\n",
      "epoch:2700\tloss:0.83969\n",
      "epoch:2800\tloss:0.83778\n",
      "epoch:2900\tloss:0.83584\n",
      "epoch:3000\tloss:0.83372\n",
      "epoch:3100\tloss:0.83154\n",
      "epoch:3200\tloss:0.82923\n",
      "epoch:3300\tloss:2.22281\n",
      "epoch:3400\tloss:0.90200\n",
      "epoch:3500\tloss:0.86589\n",
      "epoch:3600\tloss:0.84948\n",
      "epoch:3700\tloss:0.84114\n",
      "epoch:3800\tloss:0.83627\n",
      "epoch:3900\tloss:1.28928\n",
      "epoch:4000\tloss:0.82536\n",
      "epoch:4100\tloss:0.80709\n",
      "epoch:4200\tloss:0.79840\n",
      "epoch:4300\tloss:0.79111\n",
      "epoch:4400\tloss:976.67883\n",
      "epoch:4500\tloss:0.82235\n",
      "epoch:4600\tloss:0.79329\n",
      "epoch:4700\tloss:0.78039\n",
      "epoch:4800\tloss:0.76906\n",
      "epoch:4900\tloss:0.76031\n",
      "epoch:5000\tloss:0.75892\n",
      "epoch:5100\tloss:1.88707\n",
      "epoch:5200\tloss:0.74471\n",
      "epoch:5300\tloss:0.73519\n",
      "epoch:5400\tloss:0.72846\n",
      "epoch:5500\tloss:1157.15808\n",
      "epoch:5600\tloss:0.74180\n",
      "epoch:5700\tloss:0.72837\n",
      "epoch:5800\tloss:0.72317\n",
      "epoch:5900\tloss:0.71719\n",
      "epoch:6000\tloss:0.71107\n",
      "epoch:6100\tloss:0.70562\n",
      "epoch:6200\tloss:8.07135\n",
      "epoch:6300\tloss:0.68652\n",
      "epoch:6400\tloss:0.65760\n",
      "epoch:6500\tloss:0.63626\n",
      "epoch:6600\tloss:0.95483\n",
      "epoch:6700\tloss:0.61010\n",
      "epoch:6800\tloss:0.60125\n",
      "epoch:6900\tloss:0.59191\n",
      "epoch:7000\tloss:0.57940\n",
      "epoch:7100\tloss:0.56937\n",
      "epoch:7200\tloss:1.52510\n",
      "epoch:7300\tloss:0.91711\n",
      "epoch:7400\tloss:0.76588\n",
      "epoch:7500\tloss:0.67201\n",
      "epoch:7600\tloss:0.60939\n",
      "epoch:7700\tloss:0.57131\n",
      "epoch:7800\tloss:0.54407\n",
      "epoch:7900\tloss:0.52423\n",
      "epoch:8000\tloss:0.50860\n",
      "epoch:8100\tloss:0.49439\n",
      "epoch:8200\tloss:0.48205\n",
      "epoch:8300\tloss:0.47350\n",
      "epoch:8400\tloss:0.46488\n",
      "epoch:8500\tloss:0.45994\n",
      "epoch:8600\tloss:0.45799\n",
      "epoch:8700\tloss:0.44287\n",
      "epoch:8800\tloss:0.44165\n",
      "epoch:8900\tloss:110.65240\n",
      "epoch:9000\tloss:0.44420\n",
      "epoch:9100\tloss:0.43601\n",
      "epoch:9200\tloss:338.30646\n",
      "epoch:9300\tloss:0.43875\n",
      "epoch:9400\tloss:0.42939\n",
      "epoch:9500\tloss:0.42499\n",
      "epoch:9600\tloss:0.42185\n",
      "epoch:9700\tloss:0.41987\n",
      "epoch:9800\tloss:0.48615\n",
      "epoch:9900\tloss:0.42205\n"
     ]
    }
   ],
   "source": [
    "fit(X=X_train,y=y_train,n=10000,keep_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use model to predict valid_data\n",
    "def predict(X,y,keep_prob):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # restore saver\n",
    "        saver = tf.train.import_meta_graph(meta_graph_or_file=\"./nn_model/nn.model-10000.meta\")\n",
    "        model_file = tf.train.latest_checkpoint(checkpoint_dir=\"nn_model\")\n",
    "        saver.restore(sess=sess,save_path=model_file)\n",
    "\n",
    "        # init graph\n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        # get placeholder from graph\n",
    "        xs = graph.get_tensor_by_name(\"inputs:0\")\n",
    "        ys = graph.get_tensor_by_name(\"y_true:0\")\n",
    "        keep_prob_s = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "\n",
    "        # get operation from graph\n",
    "        pred = graph.get_tensor_by_name(\"pred:0\")\n",
    "\n",
    "        # run pred\n",
    "        feed_dict = {xs: X, ys: y, keep_prob_s: keep_prob}\n",
    "        y_pred = sess.run(pred,feed_dict=feed_dict)\n",
    "\n",
    "    return y_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot add op with name layer_1/Weights/weights/Adam as that name is already used",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c907279cfccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-5d912dc454c2>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X, y, keep_prob)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# restore saver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./nn_model/nn.model-10000.meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nn_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchao/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1925\u001b[0m                                       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m                                       \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1927\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m   1928\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saver_def\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchao/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mimport_scoped_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\u001b[0m\n\u001b[1;32m    739\u001b[0m     importer.import_graph_def(\n\u001b[1;32m    740\u001b[0m         \u001b[0minput_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_scope\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         producer_op_list=producer_op_list)\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;31m# Restores all the other collections.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchao/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 instructions)\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    434\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/Users/sunchao/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             op_def=op_def)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;31m# Maps from a node to the ops it is colocated with, if colocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchao/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3290\u001b[0m           op_def=op_def)\n\u001b[1;32m   3291\u001b[0m       self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[0;32m-> 3292\u001b[0;31m                              compute_device=compute_device)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchao/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3332\u001b[0m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3333\u001b[0m     \u001b[0;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3334\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3336\u001b[0m     \u001b[0;31m# Apply any additional attributes requested. Do not overwrite any existing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sunchao/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_add_op\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m   2916\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m         raise ValueError(\"cannot add op with name %s as that name \"\n\u001b[0;32m-> 2918\u001b[0;31m                          \"is already used\" % op.name)\n\u001b[0m\u001b[1;32m   2919\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot add op with name layer_1/Weights/weights/Adam as that name is already used"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_val, y_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
